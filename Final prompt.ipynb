{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2b587-5676-474a-a32a-be9fc04fc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was made with help of an example code from HuggingFace/Phi3mini4k page\n",
    "# Keep in mind that the following code was designed for iisys servers of Hof University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de237fdf-444e-4602-928e-9b74dfc6ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "# Choosing the model and configuring\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# During development the iisys servers sometimes stop the process, which is not ideal when processing 2k files that is usually 2h.\n",
    "# So in order to fix this problem, the log system was created to not process the same files twice\n",
    "def create_log_from_output(output_directory, log_file):\n",
    "    processed_files = set()\n",
    "    if os.path.exists(output_directory):\n",
    "        for filename in os.listdir(output_directory):\n",
    "            if filename.startswith(\"processed_\") and filename.endswith(\".txt\"):\n",
    "                original_filename = filename[len(\"processed_\"):]\n",
    "                processed_files.add(original_filename)\n",
    "    \n",
    "    with open(log_file, 'w') as log:\n",
    "        for filename in processed_files:\n",
    "            log.write(f\"{filename}\\n\")\n",
    "\n",
    "    return processed_files\n",
    "\n",
    "# A script to process all files in chosen directory\n",
    "def process_text_files(input_directory, output_directory, log_file):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    processed_files = create_log_from_output(output_directory, log_file)\n",
    "\n",
    "    all_files = [f for f in os.listdir(input_directory) if f.endswith(\".txt\")]\n",
    "    total_files = len(all_files)\n",
    "\n",
    "    files_to_process = [\n",
    "        f for f in all_files \n",
    "        if f not in processed_files\n",
    "    ]\n",
    "\n",
    "    with open(log_file, 'a') as log, tqdm(total=total_files, initial=len(processed_files), desc=\"Processing files\") as pbar:\n",
    "        for filename in files_to_process:\n",
    "            input_file_path = os.path.join(input_directory, filename)\n",
    "            output_file_path = os.path.join(output_directory, f\"processed_{filename}\")\n",
    "\n",
    "            try:\n",
    "                with open(input_file_path, 'r') as file:\n",
    "                    # inputing parsed bpmn files in order to put it into prompt\n",
    "                    content = file.read()\n",
    "                # The optimal prompt with content input in order to create a good quality prompt from our data\n",
    "                prompt = f\"Can you describe the following into a more natural and understandable flow description using names? Please fit it into one paragraph and not separate tables. The following: {content}\"\n",
    "\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "\n",
    "                output = pipe(messages, **generation_args)\n",
    "                generated_text = output[0]['generated_text']\n",
    "                # Updating the log\n",
    "                with open(output_file_path, 'w') as out_file:\n",
    "                    out_file.write(generated_text)\n",
    "\n",
    "                log.write(f\"{filename}\\n\")\n",
    "                log.flush()\n",
    "                pbar.update(1)\n",
    "\n",
    "                print(f\"Processed {filename} and saved to {output_file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Setting up log file\n",
    "logging.basicConfig(filename='bpmn_processing.log', level=logging.ERROR)\n",
    "\n",
    "# Directory for input files - parsed bpmn files\n",
    "input_directory = './input/output/'\n",
    "# Directory to save processed prompts into txt files\n",
    "output_directory = 'finished_output/'\n",
    "# Log file to keep track of processed files\n",
    "log_file = 'finished log.log'\n",
    "\n",
    "# Launching the script\n",
    "process_text_files(input_directory, output_directory, log_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
